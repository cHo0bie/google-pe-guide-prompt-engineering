% Конспект Руководства Google по Prompt Engineering (части 1–3)
% Основа: перевод на Хабре (Lee Boonstra, Google)

> Ссылки на оригинальный цикл: Часть 1, Часть 2, Часть 3. Материал структурирован и дополнен практиками.
> Весь конспект применим к любым LLM (Gemini, GPT, Claude, Llama, GigaChat).

## 0. Карта курса
- Блок A. База: модель, температура, Top‑K/Top‑P, длина вывода; Zero/One/Few‑shot; System/Role/Context; контекст; делимитеры; формат.
- Блок B. Продвинутое: Step‑back; Chain‑of‑Thought (CoT); Self‑consistency; Tree‑of‑Thoughts; ReAct; APE; код‑промптинг.
- Блок C. Практика: лучшие практики; JSON/схемы; документирование; CoT‑best‑practices; переменные; контроль длины; эксперименты.

---

## A1. Конфигурации вывода
**Длина** ограничивает число токенов в ответе — это не "краткость", а *обрезка*.  
**Температура** — степень случайности: 0 — детерминированность (для задач с единственным ответом), выше — креативность.  
**Top‑K / Top‑P** — альтернативы/надстройки к температуре: регулируют область выборки кандидатов.

Практика:
- Для CoT и задач с единственным правильным ответом — `temperature=0`.
- Для генерации вариантов/креатива — `temperature≈0.7–1.0`, увеличивайте K/P.

## A2. Базовые паттерны промптов
- **Zero/One/Few‑shot** — отсутствие/1/несколько примеров. Few‑shot особенно силён.
- **System/Role/Context** — отделяйте системную инструкцию, роль и фактический контекст.
- **Делимитеры** — используйте ```---```, <json>…</json> для явного отделения частей.
- **Требуемый формат** — JSON/CSV/таблица, со схемой или примерами.
- **Проверка/эскалация** — прописывайте политику ошибок: `если не уверен -> N/A`.

### Шаблон (универсальный)
```
[system] Ты — <роль>. Следуй формату и политике ошибок.
[role] Роль: <домен/персона>
[context] Факты: <список с датами/ссылками/ограничениями>
[task] Сделай <задача>. Верни <формат>.
```

## B1. Step‑back
Сначала просим модель вывести **общие принципы/рамки**, затем решаем конкретную задачу с учётом этих принципов. Улучшает полноту/качество для сложных вопросов.

## B2. Chain‑of‑Thought (CoT) и Self‑consistency
CoT — поощряет пошаговое рассуждение (в ряде задач повышает точность).  
Self‑consistency — запускаем несколько рассуждений, собираем «голоса» за ответ.  
Практика: держите рассуждение скрытым и извлекайте только **итог**.

## B3. Tree‑of‑Thoughts
Ветка рассуждений = состояние + оценка; расширяем несколько ветвей и выбираем лучшую (по эвристике).

## B4. ReAct
Цикл «мысль → действие → наблюдение». Поддержка внешних инструментов (поиск, калькулятор, БД). В проде — обязательно логируйте «действия».

## B5. Автоматический промпт‑инжиниринг (APE)
Генерируем N вариантов инструкций, ранжируем (BLEU/ROUGE/ручная рубрика), комбинируем лучшие, повторяем цикл.

## B6. Работа с кодом
Шаблоны: «объясни», «исправь баг», «написать тесты», «переписать стиль/язык», «предложи граничные случаи». Добавляйте **ограничения среды** (версия Python, фреймворки).

## C1. Лучшие практики
1) Давайте **примеры** (one/few‑shot).  
2) Проектируйте **просто**: коротко и понятно.  
3) Будьте **конкретны**: цель, формат, стиль, ограничения.  
4) Положительные **инструкции** чаще полезнее, чем списки запретов.  
5) Контролируйте **длину** (лимит токенов/указание длины).  
6) Используйте **переменные** в шаблонах для переиспользования.  
7) Для CoT — **temperature=0**, отделяйте финальный ответ от рассуждений.  
8) Документируйте **эксперименты** (шаблон таблицы версий промптов).

## C2. Контракты и JSON
Когда нужен структурированный вывод — давайте схему (JSON Schema/pydantic). Добавляйте «repair‑шаг» и политику ошибок: `"если невалидно — повтори генерацию"`.

## C3. Оценка и журналирование
- Ручная рубрика (полнота, точность, стиль, безопасность).
- Сравнение A/B мини‑тестами.
- Логи параметров: модель/температура/версия промпта/время.

---

# Практикум

### P‑01. Мини‑выгрузка
Извлеките из текста банковские атрибуты (счёт, карта, дата, сумма) в JSON по схеме. Повторите 3 итерации.

### P‑02. Генерация идей (Step‑back)
В сегменте SME предложите 5 стратегий уменьшения churn. Используйте Step‑back; сформируйте план экспериментов.

### P‑03. Классификация с Few‑shot
Разметьте обращения в саппорт на типы: платежи/карты/подписки/другое. Добавьте 3 примера на класс, измерьте точность на 20 кейсах.

### P‑04. ReAct‑протокол
Спроектируйте 2 внешних действия (поиск по базе FAQ, калькулятор комиссий) и маршрут для них. Продумайте логи.

### P‑05. APE для инструкций
Сгенерируйте 20 вариантов системной инструкции для RAG‑бота. Выберите топ‑5 по рубрике (релевантность, ясность, безопасность).

---

# Источники
- Часть 1 (Хабр): основы, параметры и базовые техники.
- Часть 2 (Хабр): Step‑back, CoT, Self‑consistency, ToT, ReAct, APE, код.
- Часть 3 (Хабр): лучшие практики, CoT‑best, переменные, документирование.
- Оригинал и материалы Google (Prompt Engineering for Generative AI).


---

## D. Банковский домен: шаблоны, чек-листы и тесты
- Шаблоны с цитированием: `prompts/banking/faq_with_citations.md`
- Строгие JSON-контракты: `prompts/banking/extract_requisites.md`, `prompts/banking/complaint_triage.md`
- Схемы: `banking/schemas/*.json`
- Чек-лист relevancy/faithfulness: `banking/checklists/rag_relevancy_checklist.md`
- Негативные тесты: `banking/negative_tests/faq_negatives.md`

### Практика (банкинг)
1) Извлечь реквизиты в JSON и провалидировать: `scripts/run.py chat --prompt-file prompts/banking/extract_requisites.md --var text='Счёт 40817810..., получатель ООО Ромашка' --schema banking/schemas/requisites_schema.json`
2) Классифицировать жалобу и выдать план действий: `scripts/run.py chat --prompt-file prompts/banking/complaint_triage.md --var complaint='Списали комиссию дважды' --schema banking/schemas/complaint_schema.json`
3) Ответить на FAQ строго по пассажам (с цитатами): `scripts/run.py chat --prompt-file prompts/banking/faq_with_citations.md --var question='Как перевыпустить карту?' --var passages='[1] ...\n[2] ...'`
